---
description: 
globs: 
alwaysApply: true
---

# Detailed Analysis: Batch Caption Job Queue Implementation

## Database Structure Analysis

### Database Approach Options:

1. **Separate Database**
   - Pros: Complete isolation, independent scaling, no risk of job tables interfering with core data
   - Cons: Additional connection management, more complex setup, potential cross-DB transaction issues
   
2. **Separate Schema in Same Database**
   - Pros: Logical separation while maintaining single connection, easier transactions
   - Cons: Still shares DB resources, potential for naming conflicts

3. **Same Database, Distinguished Naming**
   - Pros: Simplest implementation, single connection pool, easier maintenance
   - Cons: Less isolation, requires careful naming conventions

**Recommendation:** Option 2 - Create a separate schema `job_queue` within the existing PostgreSQL database.

```sql
CREATE SCHEMA job_queue;
```

## Data Service Implementation with Hono

### 1. Schema Design with Drizzle ORM

```typescript
// src/schema/job_queue.ts
import { pgTable, serial, text, timestamp, integer, json, boolean } from 'drizzle-orm/pg-core';

export const jobStatus = ['pending', 'running', 'completed', 'failed', 'cancelled'] as const;

export const captionJobs = pgTable('caption_jobs', {
  id: serial('id').primaryKey(),
  jobId: text('job_id').notNull().unique(), // UUID for the job
  status: text('status', { enum: jobStatus }).notNull().default('pending'),
  created_at: timestamp('created_at').defaultNow().notNull(),
  started_at: timestamp('started_at'),
  completed_at: timestamp('completed_at'),
  type: text('type').notNull(), // MULTI_PERSPECTIVE, DATASET_PERSPECTIVE, etc.
  priority: integer('priority').default(100).notNull(),
  total_images: integer('total_images').notNull(),
  processed_images: integer('processed_images').default(0).notNull(),
  failed_images: integer('failed_images').default(0).notNull(),
  progress: integer('progress').default(0).notNull(), // 0-100
  config: json('config').notNull(), // JSON configuration for the job
  user_id: text('user_id'), // Optional user ID
  archived: boolean('archived').default(false).notNull(),
  archive_date: timestamp('archive_date'),
});

export const jobItems = pgTable('job_items', {
  id: serial('id').primaryKey(),
  job_id: text('job_id').notNull().references(() => captionJobs.jobId, { onDelete: 'cascade' }),
  image_path: text('image_path').notNull(),
  perspective: text('perspective').notNull(),
  status: text('status', { enum: jobStatus }).notNull().default('pending'),
  result: json('result'),
  error: text('error'),
  processing_time: integer('processing_time'), // in milliseconds
  started_at: timestamp('started_at'),
  completed_at: timestamp('completed_at'),
});

// Index for performance
export const jobItemsIndex = pgIndex('job_items_job_id_idx', jobItems.job_id);
```

### 2. Hono API Implementation

```typescript
// src/routes/batch-queue.ts
import { Hono } from 'hono';
import { zValidator } from '@hono/zod-validator';
import { z } from 'zod';
import { v4 as uuidv4 } from 'uuid';
import { db } from '../db';
import { captionJobs, jobItems } from '../schema/job_queue';
import { eq, and, not } from 'drizzle-orm';
import { websocketServer } from '../websocket';

// Define the router
const app = new Hono();

// Validation schemas
const createJobSchema = z.object({
  type: z.enum(['MULTI_PERSPECTIVE', 'DATASET_PERSPECTIVE', 'BACKFILL', 'DEPENDENCY_CHAIN']),
  config: z.record(z.any()),
  images: z.array(z.string()).min(1),
  perspectives: z.array(z.string()).min(1),
  priority: z.number().optional(),
});

// Create a new job
app.post('/create', zValidator('json', createJobSchema), async (c) => {
  const data = c.req.valid('json');
  const jobId = uuidv4();
  
  // Create the job
  await db.insert(captionJobs).values({
    jobId,
    status: 'pending',
    type: data.type,
    priority: data.priority || 100,
    total_images: data.images.length * data.perspectives.length,
    config: data.config,
  });
  
  // Create job items for each image/perspective combination
  const itemValues = [];
  for (const imagePath of data.images) {
    for (const perspective of data.perspectives) {
      itemValues.push({
        job_id: jobId,
        image_path: imagePath,
        perspective,
        status: 'pending',
      });
    }
  }
  
  await db.insert(jobItems).values(itemValues);
  
  // Notify websocket clients
  websocketServer.broadcast(JSON.stringify({
    type: 'JOB_CREATED',
    jobId,
  }));
  
  return c.json({ jobId });
});

// List active jobs
app.get('/list', async (c) => {
  const jobs = await db.select()
    .from(captionJobs)
    .where(
      and(
        not(eq(captionJobs.archived, true)),
        not(eq(captionJobs.status, 'cancelled')),
      )
    )
    .orderBy(captionJobs.priority, captionJobs.created_at);
  
  return c.json({ jobs });
});

// Get detailed job status
app.get('/status/:jobId', async (c) => {
  const jobId = c.req.param('jobId');
  
  const [job] = await db.select().from(captionJobs).where(eq(captionJobs.jobId, jobId));
  
  if (!job) {
    return c.json({ error: 'Job not found' }, 404);
  }
  
  const items = await db.select().from(jobItems).where(eq(jobItems.job_id, jobId));
  
  return c.json({ 
    job,
    items,
  });
});

// Cancel a job
app.post('/cancel/:jobId', async (c) => {
  const jobId = c.req.param('jobId');
  
  const [job] = await db.select().from(captionJobs).where(eq(captionJobs.jobId, jobId));
  
  if (!job) {
    return c.json({ error: 'Job not found' }, 404);
  }
  
  if (job.status === 'completed' || job.status === 'cancelled') {
    return c.json({ error: 'Cannot cancel completed or already cancelled job' }, 400);
  }
  
  await db.update(captionJobs)
    .set({ status: 'cancelled' })
    .where(eq(captionJobs.jobId, jobId));
    
  await db.update(jobItems)
    .set({ status: 'cancelled' })
    .where(
      and(
        eq(jobItems.job_id, jobId),
        eq(jobItems.status, 'pending')
      )
    );
  
  // Notify websocket clients
  websocketServer.broadcast(JSON.stringify({
    type: 'JOB_CANCELLED',
    jobId,
  }));
  
  return c.json({ success: true });
});

// Reorder jobs
app.post('/reorder', zValidator('json', z.object({
  jobIds: z.array(z.string()),
})), async (c) => {
  const { jobIds } = c.req.valid('json');
  
  // Update priority based on array order
  for (let i = 0; i < jobIds.length; i++) {
    await db.update(captionJobs)
      .set({ priority: i * 10 })
      .where(eq(captionJobs.jobId, jobIds[i]));
  }
  
  // Notify websocket clients
  websocketServer.broadcast(JSON.stringify({
    type: 'QUEUE_REORDERED',
    jobIds,
  }));
  
  return c.json({ success: true });
});

export default app;
```

### 3. WebSocket Implementation

```typescript
// src/websocket.ts
import { Server } from 'ws';
import { createServer } from 'http';

// Create HTTP server
const server = createServer();

// Create WebSocket server
export const websocketServer = new Server({ 
  server,
  path: '/api/ws/job-queue'
});

// Handle connections
websocketServer.on('connection', (socket) => {
  console.log('Client connected to job queue websocket');
  
  socket.on('message', (message) => {
    try {
      const data = JSON.parse(message.toString());
      // Handle client messages if needed
    } catch (error) {
      console.error('Invalid websocket message', error);
    }
  });
  
  socket.on('close', () => {
    console.log('Client disconnected from job queue websocket');
  });
});

// Start server
export const startWebSocketServer = (port = 32551) => {
  server.listen(port, () => {
    console.log(`WebSocket server running on port ${port}`);
  });
};
```

### 4. Job Processor Service

```typescript
// src/services/job-processor.ts
import { db } from '../db';
import { captionJobs, jobItems } from '../schema/job_queue';
import { eq, and } from 'drizzle-orm';
import { websocketServer } from '../websocket';

// Configuration
const MAX_CONCURRENT_JOBS = 2;  // Configurable via env var
const MAX_CONCURRENT_ITEMS = 4; // Configurable via env var

// Process the queue
export async function processQueue() {
  // Count running jobs
  const runningJobsCount = await db
    .select({ count: db.fn.count() })
    .from(captionJobs)
    .where(eq(captionJobs.status, 'running'));
  
  const runningCount = Number(runningJobsCount[0].count);
  
  if (runningCount >= MAX_CONCURRENT_JOBS) {
    return; // Already at max capacity
  }
  
  // Get next pending job
  const pendingJobs = await db
    .select()
    .from(captionJobs)
    .where(eq(captionJobs.status, 'pending'))
    .orderBy(captionJobs.priority, captionJobs.created_at)
    .limit(MAX_CONCURRENT_JOBS - runningCount);
  
  if (pendingJobs.length === 0) {
    return; // No pending jobs
  }
  
  // Start processing jobs
  for (const job of pendingJobs) {
    startJob(job.jobId);
  }
}

// Start processing a job
async function startJob(jobId: string) {
  // Update job status
  await db.update(captionJobs)
    .set({ 
      status: 'running',
      started_at: new Date()
    })
    .where(eq(captionJobs.jobId, jobId));
  
  // Notify about job start
  websocketServer.broadcast(JSON.stringify({
    type: 'JOB_STARTED',
    jobId,
  }));
  
  // Start processing items
  processJobItems(jobId);
}

// Process items for a job
async function processJobItems(jobId: string) {
  // Keep track of running items
  let runningItems = 0;
  let completedItems = 0;
  let failedItems = 0;
  let totalItems = 0;
  
  // Get job and item counts
  const [job] = await db
    .select()
    .from(captionJobs)
    .where(eq(captionJobs.jobId, jobId));
  
  if (!job || job.status === 'cancelled') {
    return; // Job was cancelled or doesn't exist
  }
  
  totalItems = job.total_images;
  
  // Process function for items
  const processItem = async (item: any) => {
    try {
      // Update item status
      await db.update(jobItems)
        .set({ 
          status: 'running',
          started_at: new Date()
        })
        .where(eq(jobItems.id, item.id));
      
      runningItems++;
      
      // Call inference server API to generate caption
      // This would be an actual API call to the inference server
      const result = await callInferenceServer(item.image_path, item.perspective);
      
      // Update item with result
      await db.update(jobItems)
        .set({ 
          status: 'completed',
          result,
          completed_at: new Date(),
          processing_time: Date.now() - item.started_at.getTime()
        })
        .where(eq(jobItems.id, item.id));
      
      runningItems--;
      completedItems++;
      
      // Update job progress
      const progress = Math.floor((completedItems + failedItems) / totalItems * 100);
      await updateJobProgress(jobId, completedItems, failedItems, progress);
      
      // Process next items
      processNextItems();
      
    } catch (error) {
      // Handle error
      await db.update(jobItems)
        .set({ 
          status: 'failed',
          error: String(error),
          completed_at: new Date()
        })
        .where(eq(jobItems.id, item.id));
      
      runningItems--;
      failedItems++;
      
      // Update job progress
      const progress = Math.floor((completedItems + failedItems) / totalItems * 100);
      await updateJobProgress(jobId, completedItems, failedItems, progress);
      
      // Process next items
      processNextItems();
    }
  };
  
  // Helper to get and process next items
  const processNextItems = async () => {
    // Check if job cancelled
    const [currentJob] = await db
      .select()
      .from(captionJobs)
      .where(eq(captionJobs.jobId, jobId));
    
    if (!currentJob || currentJob.status === 'cancelled') {
      return; // Job was cancelled
    }
    
    // Check if all items are processed
    if (completedItems + failedItems === totalItems) {
      // All items processed, update job status
      await db.update(captionJobs)
        .set({ 
          status: failedItems === totalItems ? 'failed' : (failedItems > 0 ? 'completed' : 'completed'),
          completed_at: new Date(),
        })
        .where(eq(captionJobs.jobId, jobId));
      
      // Notify about job completion
      websocketServer.broadcast(JSON.stringify({
        type: 'JOB_COMPLETED',
        jobId,
        status: failedItems === totalItems ? 'failed' : (failedItems > 0 ? 'partial' : 'success'),
      }));
      
      // Process next job in queue
      processQueue();
      return;
    }
    
    // Get next items to process
    if (runningItems < MAX_CONCURRENT_ITEMS) {
      const pendingItems = await db
        .select()
        .from(jobItems)
        .where(
          and(
            eq(jobItems.job_id, jobId),
            eq(jobItems.status, 'pending')
          )
        )
        .limit(MAX_CONCURRENT_ITEMS - runningItems);
      
      for (const item of pendingItems) {
        processItem(item);
      }
    }
  };
  
  // Start processing
  processNextItems();
}

// Update job progress
async function updateJobProgress(jobId: string, completedImages: number, failedImages: number, progress: number) {
  await db.update(captionJobs)
    .set({ 
      processed_images: completedImages,
      failed_images: failedImages,
      progress,
    })
    .where(eq(captionJobs.jobId, jobId));
  
  // Notify about progress update
  websocketServer.broadcast(JSON.stringify({
    type: 'JOB_PROGRESS',
    jobId,
    progress,
    processed_images: completedImages,
    failed_images: failedImages,
  }));
}

// Mock inference server call (replace with actual implementation)
async function callInferenceServer(imagePath: string, perspective: string) {
  // Simulating an API call to the inference server
  return new Promise((resolve, reject) => {
    setTimeout(() => {
      // 90% success rate for testing
      if (Math.random() > 0.1) {
        resolve({
          content: `Caption for ${perspective} on ${imagePath}`,
          metadata: {
            model: "test-model",
            provider: "test-provider"
          }
        });
      } else {
        reject(new Error("Caption generation failed"));
      }
    }, 2000 + Math.random() * 3000); // Simulate processing time
  });
}

// Archiving service for old jobs
export async function archiveOldJobs() {
  const sevenDaysAgo = new Date();
  sevenDaysAgo.setDate(sevenDaysAgo.getDate() - 7);
  
  await db.update(captionJobs)
    .set({ 
      archived: true,
      archive_date: new Date(),
    })
    .where(
      and(
        not(eq(captionJobs.archived, true)),
        db.lte(captionJobs.completed_at, sevenDaysAgo)
      )
    );
}

// Initialize job processor
export function initJobProcessor() {
  // Process queue every 10 seconds
  setInterval(processQueue, 10000);
  
  // Archive old jobs daily
  setInterval(archiveOldJobs, 24 * 60 * 60 * 1000);
  
  // Initial processing
  processQueue();
}
```

## Integration with Main Application

### 1. Import and register in main Hono app

```typescript
// src/index.ts
import { Hono } from 'hono';
import { cors } from 'hono/cors';
import batchQueueRoutes from './routes/batch-queue';
import { startWebSocketServer } from './websocket';
import { initJobProcessor } from './services/job-processor';

const app = new Hono();

// Middleware
app.use('*', cors());

// Routes
app.route('/api/perspectives/batch', batchQueueRoutes);

// Health check
app.get('/health', (c) => c.text('OK'));

// Start server
const port = parseInt(process.env.PORT || '32550', 10);
console.log(`Starting server on port ${port}`);

// Start WebSocket server
startWebSocketServer();

// Initialize job processor
initJobProcessor();

// Export for serverless environments
export default {
  port,
  fetch: app.fetch
};
```

### 2. Docker Updates

Add a new service to docker-compose.yml or update the existing data_service:

```yaml
graphcap_data_service:
  container_name: graphcap_data_service
  build:
    context: ./servers/data_service
    dockerfile: Dockerfile.data_service.dev
  ports:
    - "32550:32550"
    - "32551:32551"  # WebSocket port
    - "32501:32501"  # Drizzle Kit Studio port
  environment:
    - NODE_ENV=${NODE_ENV:-development}
    - PORT=32550
    - WEBSOCKET_PORT=32551
    - DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST:-graphcap_postgres}:${POSTGRES_PORT:-5432}/${POSTGRES_DB}
    - WORKSPACE_PATH=/workspace
    - DEBUG=true  # Enable debug mode
    - MAX_CONCURRENT_JOBS=2
    - MAX_CONCURRENT_ITEMS=4
  volumes:
    - ./workspace:/workspace
    - ./servers/data_service/src:/app/src  # Mount src directory directly
  networks:
    - graphcap
  depends_on:
    graphcap_postgres:
      condition: service_healthy
  healthcheck:
    test: ["CMD", "wget", "--spider", "http://localhost:32550/health"]
    interval: 5m
    timeout: 10s
    retries: 3
    start_period: 30s
```

## Advantages of This Design

1. **Database Schema Separation**: Using a separate schema for the job queue provides logical separation while maintaining connection efficiency.

2. **FIFO with Reordering**: The priority field enables both FIFO behavior (by using creation date as secondary sort) and manual reordering capability.

3. **Granular Status Tracking**: Tracking status at both job and item level allows for detailed progress reporting and partial failure handling.

4. **Real-time Updates**: WebSocket integration provides immediate status updates to the UI.

5. **Configurable Concurrency**: Parameters can be adjusted through environment variables to optimize for different server configurations.

6. **Archiving System**: Implements the 7-day retention policy with automated archiving.

7. **Error Handling**: Robust error handling at the item level allows jobs to continue even when individual captions fail.

Using these components, we now have a robust foundation for our batch captioning system that meets all the specified requirements.
